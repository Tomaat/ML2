{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Bayesian PCA\n",
    "\n",
    "### Machine Learning II, 2016\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The lab exercises should be made in groups of two people.\n",
    "* The deadline for part 1 is Sunday, 15 May, 23:59.\n",
    "* Assignment should be sent to taco.cohen at gmail dot com. The subject line of your email should be \"[MLII2016] lab3part1_lastname1\\_lastname2\". \n",
    "* Put your and your teammates' names in the body of the email\n",
    "* Attach the .IPYNB (IPython Notebook) file containing your code and answers. Naming of the file follows the same rule as the subject line. For example, if the subject line is \"[MLII2016] lab01\\_Kingma\\_Hu\", the attached file should be \"lab3part1\\_Kingma\\_Hu.ipynb\". Only use underscores (\"\\_\") to connect names, otherwise the files cannot be parsed.\n",
    "\n",
    "Notes on implementation:\n",
    "\n",
    "* You should write your code and answers in an IPython Notebook: http://ipython.org/notebook.html. If you have problems, please contact us.\n",
    "* Among the first lines of your notebook should be \"%pylab inline\". This imports all required modules, and your plots will appear inline.\n",
    "* NOTE: test your code and make sure we can run your notebook / scripts!\n",
    "* NOTE: please write your answers directly below the question in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\vline}{\\mid}$\n",
    "$\\newcommand{\\hline}{\\dfrac{\\quad}{}}$\n",
    "$\\newcommand{\\mwtilde}[1][i]{\\tilde{\\vt{m}}_{\\vt{w}}^{(#1)}}$\n",
    "$\\newcommand{\\mw}[1][k]{{\\vt{m}_{\\vt{w}}^{(#1)}}}$\n",
    "$%\\newcommand{\\E}[1]{\\mathbb{E}\\left[ #1 \\right]}$\n",
    "$\\newcommand{\\E}[1]{\\left \\langle #1 \\right\\rangle}$\n",
    "$\\newcommand{\\EE}[1]{\\left\\langle #1 \\right\\rangle}$\n",
    "$\\newcommand{\\vt}[1]{\\boldsymbol{\\mathbf{#1}}}$\n",
    "$\\newcommand{\\gaus}[1]{\\mathcal{N}\\left(#1\\right)}$\n",
    "$\\newcommand{\\lb}[0]{\\left [}$\n",
    "$\\newcommand{\\rb}[0]{\\right ]}$\n",
    "$\\newcommand{\\lp}{\\left(}$\n",
    "$\\newcommand{\\rp}{\\right)}$\n",
    "$\\newcommand{\\la}{\\left \\{}$\n",
    "$\\newcommand{\\ra}{\\right \\}}$\n",
    "\n",
    "> #### By: Ysbrand Galama, 10262067\n",
    "> #### Ties van Rozendaal, 10077391"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In this lab assignment, we will implement a variational algorithm for Bayesian PCA. Unlike regular PCA based on maximization of retained variance or minimization of projection error (see Bishop, 12.1.1 and 12.1.2), probabilistic PCA defines a proper density model over observed and latent variables. We will work with a fully Bayesian model this time, which is to say that we will put priors on our parameters and will be interested in learning the posterior over those parameters. Bayesian methods are very elegant, but require a shift in mindset: we are no longer looking for a point estimate of the parameters (as in maximum likelihood or MAP), but for a full posterior distribution over the space of parameters.\n",
    "\n",
    "The integrals involved in a Bayesian analysis are usually analytically intractable, so that we must resort to approximations. In this lab assignment, we will implement the variational method described in Bishop99. Chapters 10 and 12 of the PRML book contain additional material that may be useful when doing this exercise.\n",
    "\n",
    "* [Bishop99] Variational Principal Components, C. Bishop, ICANN 1999 - http://research.microsoft.com/pubs/67241/bishop-vpca-icann-99.pdf\n",
    "\n",
    "Below, you will find some code to get you started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The Q-distribution (5 points)\n",
    "\n",
    "In variational Bayes, we introduce a distribution $Q(\\Theta)$ over parameters / latent variables in order to make inference tractable. We can think of $Q$ as being an approximation of a certain distribution. What function does $Q$ approximate, $p(D|\\Theta)$, $p(\\Theta|D)$, $p(D, \\Theta)$, $p(\\Theta)$, or $p(D)$, and how do you see that from the equation $\\ln p(D) = \\mathcal{L}(Q) + \\mathrm{KL}(Q||P)$? (Hint: see eq. 11 in Bishop99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The function $Q$ approximates $p(\\Theta|D)$, which is what we want, to make the $KL$ as small as possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The mean-field approximation (15 points)\n",
    "\n",
    "Equation 13 from [Bishop99] is a very powerful result: assuming only that $Q(\\Theta)$ factorizes in a certain way (no assumptions on the functional form of the factors $Q_i$!), we get a set of coupled equations for the $Q_i$.\n",
    "\n",
    "However, the expression given in eq. 13 for Q_i contains a small mistake. Starting with the expression for the lower bound $\\mathcal{L}(Q)$, derive the correct expression (and include your derivation). You can proceed as follows: first, substitute the factorization of $Q$ (eq. 12) into the definition of $\\mathcal{L}(Q)$ and separate $\\mathcal{L}(Q)$ into $Q_i$-dependent and $Q_i$-independent terms. At this point, you should be able to spot the expectations $\\langle\\cdot\\rangle_{k \\neq i}$ over the other $Q$-distributions that appear in Bishop's solution (eq. 13). Now, keeping all $Q_k, k \\neq i$ fixed, maximize the expression with respect to $Q_i$. You should be able to spot the form of the optimal $\\ln Q_i$, from which $Q_i$ can easily be obtained.\n",
    "\n",
    ">\\begin{align}\n",
    "Q(\\theta) =& \\prod_i Q_i(\\theta_i) \\\\\n",
    "\\mathcal{L}(Q) =& \\int Q(\\theta) \\log\\left( \\frac{P(D, \\theta)}{Q(\\theta)} \\right) d\\theta \\\\\n",
    "=& \\int Q(\\theta) \\log P(X,\\theta) d\\theta - \\int Q(\\theta) \\log Q(\\theta) d\\theta \\\\\n",
    "=& \\int \\prod_i Q_i(\\theta_i) \\log P(X,\\theta) d\\theta - \\int \\prod_i Q)i(\\theta_i) \\log \\prod_j Q_j(\\theta_j) d\\theta \\\\\n",
    "=& \\int Q_i(\\theta_i) \\prod_{k\\neq i} Q_k(\\theta_k) \\log P(X,\\theta) d\\theta - \\int \\prod_i Q_i(\\theta_i) \\sum_j \\log Q_j(\\theta_j) d\\theta \\\\\n",
    "=& \\int Q_i(\\theta_i) \\int \\prod_{k\\neq i} Q_k(\\theta_k) \\log P(X,\\theta) d\\theta_{\\backslash i} d\\theta_i - \\sum_j \\int \\prod_i Q_i(\\theta_i) \\log Q_j(\\theta_j) d\\theta \\\\\n",
    "=& \\int Q_i(\\theta_i) \\EE{ \\log P(X,\\theta) }_{k\\neq i} d\\theta_i - \\int Q_i(\\theta_i)\\log Q_i(\\theta_i) - \\sum_{j\\neq i} \\int Q_j(\\theta_j) \\log Q_j(\\theta_j) d\\theta \\\\\n",
    "=& \\int Q_i(\\theta_i) \\EE{ \\log P(X,\\theta) }_{k\\neq i} d\\theta_i - \\E{\\log Q_i(\\theta_i)} - \\EE{ \\log p(D,\\theta) }_{j\\neq i} \\\\\n",
    "max\\; Q_i(\\theta_i) \\Rightarrow& \\\\\n",
    "\\log Q_i(\\theta_i) =& - \\int Q_i(\\theta_i) \\EE{ \\log P(X,\\theta) }_{k\\neq i} d\\theta_i + \\EE{ \\log P(D,\\theta) }_{k\\neq i} \\\\\n",
    "Q_i(\\theta_i) =& \\frac{ \\exp \\EE{ \\log P(D,\\theta) }_{k\\neq i}  }{\\int \\exp Q_i(\\theta_i) \\EE{ \\log P(X,\\theta) }_{k\\neq i} d\\theta_i}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. The log-probability (10 points)\n",
    "\n",
    "Write down the log-prob of data and parameters, $\\ln p(\\mathbf{X}, \\mathbf{Z}, \\mathbf{W}, \\mathbf{\\alpha}, \\tau, \\mathbf{\\mu})$, in full detail (where $\\mathbf{X}$ are observed, $\\mathbf{Z}$ is latent; this is different from [Bishop99] who uses $\\mathbf{T}$ and $\\mathbf{X}$ respectively, but $\\mathbf{X}$ and $\\mathbf{Z}$ are consistent with the PRML book and are more common nowadays). Could we use the log-prob to assess the convergence of the variational Bayesian PCA algorithm? If yes, how? If no, why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> $$\\begin{align}\n",
    "\\ln p(\\vt{X}, \\vt{Z}, \\vt{W}, \\vt{\\alpha}, \\tau, \\vt{\\mu})\n",
    "=& \\ln p(\\vt{X}) p(\\vt{X}|, \\vt{Z}, \\vt{\\mu}, \\tau)p(\\vt{W}| \\vt{\\alpha})p(\\vt{\\alpha})p(\\vt{\\mu})p(\\tau) \\\\\n",
    "=& \\sum_{n=1}^N  \\lb \\ln p(\\vt{x}_n) + \\ln p(\\vt{x}_n|, \\vt{z}_n, \\vt{\\mu}, \\tau)+ \\ln p(\\vt{W}| \\vt{\\alpha})+ \\ln p(\\vt{\\mu})+ \\ln p(\\vt{\\alpha})+ \\ln p(\\tau) \\rb \\\\\n",
    "=& \\sum_{n=1}^N \\ln \\gaus{\\vt{x}_n|\\vt{0},\\vt{I}_q} + \\sum_{n=1}^N \\ln \\gaus{\\vt{t}_n | \\vt{Wx}_n+\\vt{\\mu},\\tau^{-1}\\vt{I}_{d}} \\\\\n",
    "& + N \\sum_{i=1}^q \\ln \\left ( \\dfrac{\\alpha_i}{2 \\pi} \\right ) ^ {d/2} \\exp \\left \\{  -\\dfrac{1}{2} \\alpha_i||\\vt{w}_i||^2\n",
    "\\right \\}\n",
    "+ N \\ln \\gaus{\\vt{\\mu} | \\vt{0},\\beta^{-1}\\vt{I}_d}\n",
    "+ N \\sum_{i=1}^q \\ln \\Gamma (\\alpha_i|a_{\\alpha},b_{\\alpha})\n",
    "+ N \\ln \\Gamma (\\tau | c_{\\tau}, d_{\\tau})\n",
    "\\end{align}$$\n",
    "\n",
    "> We want to optimise the posterior probability. We could find the posterior from the joint (using bayes theorem), but this is intractable. $Q$ is an approximation of the posterior, with different parameters. Therefore we cannot check the convergence of $Q$ (which is what we do in the bayesian PCA) using the joint $p(\\vt{X}, \\vt{Z}, \\vt{W}, \\vt{\\alpha}, \\tau, \\vt{\\mu})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. The lower bound $\\mathcal{L}(Q)$ (25 points)\n",
    "\n",
    "Derive an expression for the lower bound $\\mathcal{L}(Q)$ of the log-prob $\\ln p(X)$ for Bayesian PCA, making use of the factorization (eq. 12) and the form of the Q-distributions (eq. 16-20) as listed in [Bishop99]. Show your steps. Implement this function.\n",
    "\n",
    "The following result may be useful:\n",
    "\n",
    "For $x \\sim \\Gamma(a,b)$, we have $\\langle \\ln x\\rangle = \\ln b + \\psi(a)$, where $\\psi(a) = \\frac{\\Gamma'(a)}{\\Gamma(a)}$ is the digamma function (which is implemented in numpy.special)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ">$\\begin{align}\n",
    "\\mathcal{L}(W,\\alpha,\\tau,\\mu) =& \\int\\int\\int\\int\\int Q(Z,W,\\alpha\\tau,\\mu) \\log\\left\\{ \\frac{p(X,Z,W,\\alpha,\\tau,\\mu)}{Q(Z,W,\\alpha,\\tau,\\mu)}\\right\\} dZdWd\\alpha d\\tau d\\mu \\\\\n",
    "=& \\E{\\log p(X,Z,W,\\alpha,\\tau,\\mu)} - \\E{\\log p(Z,W,\\alpha,\\tau,\\mu)} \\\\\n",
    "=& \\E{\\log p(X|Z,W,\\mu)} + \\E{\\log p(Z)} + \\E{\\log p(W|\\alpha)} + \\E{\\log p(\\alpha)} + \\E{\\log p(\\tau)} + \\E{\\log p(\\mu)} \\\\&- \\E{\\log Q(Z)} - \\E{\\log Q(W)} - \\E{\\log Q(\\alpha)} - \\E{\\log Q(\\tau)} - \\E{\\log Q(\\mu)}\n",
    "\\end{align}$\n",
    "\n",
    "> Where all the expectations are with respect to the random variables they involve. We will derive these expectations below. (We do often omit the notation with respect to what we take the expectation, this will allways be all the random variable(s) involved.)\n",
    "\n",
    ">$\\begin{align}\n",
    "\\E{\\log p(X|Z,W,\\mu)} &= \\E{\\log \\prod^N_{n=1} \\gaus{x_n|Wz + \\mu, \\tau^{-1}I_d} }  \\\\\n",
    "&= \\sum^N_{n=1} \\E{\\log(2\\pi)^{-\\frac12} + \\log|\\tau^{-1} I_d|^{-\\frac d2} - \\frac12(\\vt{x}_n - \\vt{Wz}_n -\\vt{\\mu})^T\\tau I_d(\\vt{x}_n - \\vt{Wz}_n -\\vt{\\mu})}\\\\\n",
    "&= -\\frac12\\log(2\\pi) -\\E{\\frac d2\\log \\tau^{-d}} -\\frac12 \\sum^N_{n=1} \\E{\\tau}\\E{(\\vt{x}_n - \\vt{Wz}_n -\\vt{\\mu})^T\\ (\\vt{x}_n - \\vt{Wz}_n -\\vt{\\mu} )}\\\\\n",
    "&= -\\frac12\\log(2\\pi) +\\E{\\frac {d^2}2\\log \\tau} \\\\\n",
    "& \\quad -\\frac12 \\E{\\tau} \\sum^N_{n=1} \\E{ \\vt{x}_n^T \\vt{x}_n -2 \\vt{x}_n^T \\lp \\vt{Wz}_n + \\vt{\\mu} \\rp + 2\\vt{z}_n^T\\vt{W}^T\\vt{\\mu}+ \\vt{z}_n^T\\vt{W}^T\\vt{Wz}_n + \\vt{\\mu}^T\\vt{\\mu} }\\\\\n",
    "&= -\\frac12\\log(2\\pi) +\\frac {d^2}2 \\E{\\log \\tau} \\\\\n",
    "& \\quad -\\frac12 \\E{\\tau} \\sum^N_{n=1} \\lp \\E{ \\vt{x}_n^T \\vt{x}_n} -2 \\E{\\vt{x}_n}^T \\lp \\E{\\vt{W}}\\E{\\vt{z}_n} + \\E{\\vt{\\mu}} \\rp + 2\\E{\\vt{z}_n}^T\\E{\\vt{W}}^T\\E{\\vt{\\mu}}+ \\E{\\vt{z}_n^T\\vt{W}^T\\vt{Wz}_n} + \\E{\\vt{\\mu}^T\\vt{\\mu}}  \\rp \\\\\n",
    "&= -\\frac12\\log(2\\pi) +\\frac {d^2}2 \\E{\\log \\tau} \\\\\n",
    "& \\quad -\\frac12 \\E{\\tau} \\sum^N_{n=1} \\lp ||\\vt{x}_n||^2 -2 \\vt{x}_n^T \\lp \\E{\\vt{W}}\\E{\\vt{z}_n} + \\E{\\vt{\\mu}} \\rp + 2\\E{\\vt{z}_n}^T\\E{\\vt{W}}^T\\E{\\vt{\\mu}}+ \\E{\\vt{z}_n^T\\vt{W}^T\\vt{Wz}_n} + \\E{\\vt{\\mu}^T\\vt{\\mu}}  \\rp \n",
    "\\end{align}\n",
    "$\n",
    "$\n",
    "\\begin{align}\n",
    "\\E{\\vt{z}_n^T\\vt{W}^T\\vt{W}\\vt{z}_n} &= \\E{\\vt{z}_n^T\\E{\\vt{W}^T\\vt{W}}_{Q(\\vt{W})}\\vt{z}_n}_{Q(\\vt{z}_n)} \\\\\n",
    "&= Tr \\lp \\E{\\vt{W}^T\\vt{W}}  Cov \\lp \\vt{z}_n \\rp \\rp + \\E{\\vt{z}_n}^T \\E{\\vt{W}^T\\vt{W}} \\E{\\vt{z}_n}\n",
    "&& \\text{(Cookbook 328)} \\\\\n",
    "&= Tr \\lp \\E{\\vt{W}^T\\vt{W}}  \\vt{\\Sigma_z} \\rp + \\E{\\vt{z}_n}^T \\E{\\vt{W}^T\\vt{W}} \\E{\\vt{z}_n} \\\\\n",
    "\\E{\\log p(Z)} &= \\E{\\log \\prod^N_{n=1} \\gaus{\\vt{z_n} | \\vt{0},\\vt{I}_q} } \\\\\n",
    "&= \\sum^N_{n=1} \\E{\\log \\la (2\\pi)^{-\\frac12} |I_q|^{-\\frac q2}\\exp\\lp -\\frac12(\\vt{z}_n-\\vt{0})^T(\\vt{z}_n-\\vt{0})  \\rp \\ra } \\\\\n",
    "&= -\\frac12 \\Big [ N \\log(2\\pi)  +\\sum^N_{n=1} \\E{\\vt{z}_n^T\\vt{z}_n} \\Big ]\\\\\n",
    "\\E{\\log p(W | \\alpha)}\n",
    "&= \\E{ \\ln \\prod_{i=1}^q  \\left ( \\dfrac{\\alpha_i}{2 \\pi} \\right ) ^ {d/2} \\exp \\left \\{  -\\dfrac{1}{2} \\alpha_i||\\vt{w}_i||^2 \\right \\} } \\\\\n",
    "&= \\E{  \\sum_{i=1}^q  \\left \\{\\dfrac{d}{2}\\ln \\alpha_i -\\dfrac{d}{2} \\ln (2 \\pi)   -\\dfrac{1}{2} \\alpha_i||\\vt{w}_i||^2 \\right \\} } \\\\\n",
    "&=  \\dfrac{d}{2} \\sum_{i=1}^q \\E{\\ln \\alpha_i}_{Q(\\alpha_i)} -\\dfrac{d}{2} q \\ln (2 \\pi) -\\dfrac{1}{2} \\sum_{i=1}^q  \\E{\\alpha_i}_{Q(\\alpha_i)} \\E{   ||\\vt{w}_i||^2  }_{Q{\\vt{w}_i}} \\\\\n",
    "&=  \\dfrac{1}{2}\\lb d \\E{\\ln \\vt{\\alpha}} -d q \\ln (2 \\pi) - \\sum_{i=1}^q  \\E{\\alpha_i} \\E{   \\vt{w}_i^T\\vt{w}_i  } \\rb \\\\\n",
    "\\E{\\vt{w}_i} &= \n",
    "\\begin{pmatrix}\n",
    "{m_w^{(1)}}_i \\\\\n",
    "\\vdots \\\\\n",
    "{m_w^{(d)}}_i\n",
    "\\end{pmatrix} =  \\mwtilde && \\text{(we define } \\mwtilde \\text{ for shorter notation)}\\\\\n",
    "\\E{||\\vt{w}_i||^2} &= \\E{\\vt{w}_i^T\\vt{w}_i} = Tr(Var(\\vt{w}_i)) + \\E{\\vt{w}_i}^T\\E{\\vt{w}_i}\n",
    "&&\\text{(318 cookbook)}\\\\\n",
    "&=  \\sum_{j=1}^d var(w_{ij}) + ||\\mwtilde[i]||^2 \\\\\n",
    "&=  Tr(\\vt{\\Sigma_w}) + ||\\mwtilde[i]||^2 \\\\\n",
    "\\E{\\alpha_i} &= \\dfrac{a_{\\alpha}}{b_{\\alpha i}} \\\\\n",
    "\\E{\\log p(\\vt{\\alpha})} &= \\E {\\sum_{i=1}^q \\ln \\Gamma (\\alpha_i|a_{\\alpha},b_{\\alpha})} \\\\\n",
    "&= \\sum_{i=1}^q H\\left[\\Gamma (\\alpha_i|a_{\\alpha},b_{\\alpha})\\right] \\\\\n",
    "&= q \\cdot \\Big( \\ln \\Gamma(a_\\alpha) - (a_\\alpha -1)\\psi(a_\\alpha) - \\ln b_{\\alpha} +a_{\\alpha} \\Big)\\\\\n",
    "\\E{\\log p(\\tau)} &= \\E{\\log \\Gamma (\\tau | c_{\\tau}, d_{\\tau})} \\\\\n",
    "&= H\\left[\\Gamma (\\tau | c_{\\tau}, d_{\\tau})\\right] \\\\\n",
    "&= \\ln \\Gamma(c_\\tau) - (c_\\tau -1)\\psi(c_\\tau) - \\ln d_{\\tau} +c_{\\tau} \\\\\n",
    "\\E{\\log p(\\mu)} &= \\E{\\log \\gaus{\\vt{\\mu} | \\vt{0},\\beta^{-1}\\vt{I}_d} } \\\\\n",
    "&= \\E{\\log (2\\pi)^{-\\frac12} |\\beta^{-1}I_d|^{-\\frac d2}\\exp\\lp -\\frac12(\\mu-0)^T\\beta I_d(\\mu-0) \\rp } \\\\\n",
    "&= \\frac12 \\bigg [ d^2\\log \\beta -\\log(2\\pi)  -\\beta\\E{\\vt{\\mu}^T \\vt{\\mu}} \\bigg ] \\\\\n",
    "\\E{\\log Q(Z)} &= \\E{\\log \\prod^N_{n=1} \\gaus{z_n|m_z^{(n)},\\Sigma_z}} \\\\\n",
    "&= \\sum^N_{n=1} \\E{\\log \\gaus{z_n|m_z^{(n)},\\Sigma_z}} \\\\\n",
    "&= \\sum^N_{n=1} H\\left[\\gaus{x_z|m_z^{(n)},\\Sigma_z}\\right]\n",
    "&& \\text{(entropy)}\\\\\n",
    "&= \\sum^N_{n=1} \\lp \\frac12 \\log |\\Sigma_z| + \\frac q 2(1+\\log(2\\pi)) \\rp\n",
    "&& \\text{(Bishop appendix B)}\\\\\n",
    "&= \\frac{N}{2} \\Big [ \\log |\\Sigma_z| +  q(1+\\log(2\\pi)) \\Big ] \\\\\n",
    "\\E{\\log Q(W)} &= \\E{\\log \\prod^d_{k=1} \\gaus{\\tilde{w}_k|m_w^{(k)},\\Sigma_w}} \\\\\n",
    "&= \\sum^d_{k=1} \\E{\\log \\gaus{\\tilde{w}_k|m_w^{(k)},\\Sigma_w}} \\\\\n",
    "&= \\sum^d_{k=1} H\\left[\\gaus{\\tilde{w}_k|m_w^{(k)},\\Sigma_w}\\right]\n",
    "&& \\text{(entropy)}\\\\\n",
    "&= \\sum^d_{k=1} \\lp \\frac12 \\log |\\Sigma_w| + \\frac q2(1+\\log(2\\pi)) \\rp\n",
    "&& \\text{(Bishop appendix B)}\\\\\n",
    "&= \\dfrac{d}{2} \\Big [ \\log |\\Sigma_w| + q (1+\\log(2\\pi)) \\Big ] \\\\\n",
    "\\E{\\log Q(\\alpha)} &= \\E{\\log \\prod^q_{i=1}\\Gamma(\\alpha_i|\\tilde{a}_\\alpha,\\tilde{b}_{\\alpha})} \\\\\n",
    "&= \\sum_{i=1}^q \\Big( \\ln \\Gamma(\\tilde{a}_\\alpha) - (\\tilde{a}_\\alpha -1)\\psi(\\tilde{a}_\\alpha) - \\ln \\tilde{b}_{\\alpha i} +\\tilde{a}_{\\alpha} \\Big)\\\\\n",
    "\\E{\\log Q(\\tau)} &= \\E{\\log \\Gamma(\\tau|\\tilde{a}_\\tau,\\tilde{b}_\\tau) } \\\\\n",
    "&= \\ln \\Gamma(\\tilde{a}_\\tau) - (\\tilde{a}_\\tau -1)\\psi(\\tilde{a}_\\tau) - \\ln \\tilde{b}_{\\tau} +\\tilde{a}_{\\tau} \\\\\n",
    "\\E{\\log Q(\\mu)} &= \\E{\\log \\gaus{\\mu|m_\\mu,\\Sigma_\\mu}} \\\\\n",
    "&= H\\left[ \\gaus{\\mu|m_\\mu,\\Sigma_\\mu} \\right]\n",
    "&& \\text{(entropy)}\\\\\n",
    "&= \\frac12 \\Big [ \\log |\\Sigma_\\mu| + d(1+\\log(2\\pi)) \\Big ]\n",
    "&& \\text{(Bishop appendix B)}\\\\\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Optimize variational parameters (50 points)\n",
    "Implement the update equations for the Q-distributions, in the __update_XXX methods. Each update function should re-estimate the variational parameters of the Q-distribution corresponding to one group of variables (i.e. either $Z$, $\\mu$, $W$, $\\alpha$ or $\\tau$).\n",
    "\n",
    "Hint: if you run into numerical instabilities resulting from the gamma function use the gammaln function from numpy.special."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In order to compute the update method, we need the following expectations. We find them by the definition of the expectations of the distributions specified in Bishop 16-20.\n",
    "\n",
    "> For the expectations of $Q(X), Q(\\vt{W}), Q(\\vt{\\alpha})$, we make fact of the use that the distributions of their components are i.i.d. and we can thus take the expectation for their components individually.\n",
    "\n",
    "> $$\n",
    "\\begin{align}\n",
    "\\E{\\tau} &= \\dfrac{\\widetilde{a}_{\\tau}}{\\widetilde{b}_{\\tau}}\\\\\n",
    "\\E{\\vt{z}_n} &= \\vt{m_z}^{(n)}\\\\\n",
    "\\E{\\vt{z}_n\\vt{z}_n^T} &= Cov(\\vt{z}_n) + \\E{\\vt{z}_n}\\E{\\vt{z}_n}^T &&\\text{(321 cookbook)}\\\\\n",
    "&= \\vt{\\Sigma_z} + \\vt{m_z}^{(n)}{\\vt{m_z}^{(n)}}^T \\\\\n",
    "\\E{||\\vt{z}_n||^2} &= \\E{\\vt{z}_n^T\\vt{z}_n} \\\\\n",
    "&= Tr(Var(\\vt{z}_n)) + \\E{\\vt{z}_n}^T\\E{\\vt{z}_n} &&\\text{(318 cookbook)}\\\\\n",
    "&= Tr(\\vt{\\Sigma_z}) + {\\vt{m_z}^{(n)} } ^T \\vt{m_z}^{(n)} \\\\\n",
    "\\E{\\vt{\\mu}} &= \\vt{m_{\\mu}}\\\\\n",
    "\\E{\\vt{\\mu}^T} &= \\E{\\vt{\\mu}}^T = \\vt{m_{\\mu}}^T\\\\ \n",
    "\\E{||\\vt{\\mu}||^2} &= \\E{\\vt{\\mu}^T\\vt{\\mu}} = Tr(Var(\\vt{\\mu})) + \\E{\\vt{\\mu}}^T\\E{\\vt{\\mu}} &&\\text{(318 cookbook)}\\\\\n",
    "&= Tr(\\vt{\\Sigma_{\\mu}})) + \\vt{m_{\\mu}}^T\\vt{m_{\\mu}} \\\\\n",
    "\\E{\\mu_k} &= {m_{\\mu}}_k\\\\\n",
    "\\E{\\vt{\\alpha}} &= \\begin{pmatrix}\n",
    " \\dfrac{ \\widetilde{a}_{\\alpha} }{\\widetilde{b}_{\\alpha 1} }\\\\\n",
    " \\vdots \\\\\n",
    " \\dfrac{ \\widetilde{a}_{\\alpha} }{\\widetilde{b}_{\\alpha q} } \\\\\n",
    " \\end{pmatrix}\n",
    " = \\widetilde{a}_{\\alpha} \\vt{\\tilde{b}}_{\\alpha}^{-1} &&\\text{(elementwise inverse)} \\\\\n",
    " \\E{\\vt{W}}\n",
    " &=\n",
    "  \\begin{pmatrix}\n",
    "\\hline & {\\mw[1]}^T & \\hline\\\\ \n",
    " & \\vdots & \\\\\n",
    "\\hline & {\\mw[d]}^T & \\hline\\\\ \n",
    "\\end{pmatrix} \\\\\n",
    " \\E{\\vt{W}^T} &=  \\E{\\vt{W}}^T \\\\\n",
    " &=\n",
    "  \\begin{pmatrix}\n",
    "\\hline & {\\mw[1]}^T & \\hline\\\\ \n",
    " & \\vdots & \\\\\n",
    "\\hline & {\\mw[d]}^T & \\hline\\\\ \n",
    "\\end{pmatrix} ^T \\\\\n",
    "&=\n",
    " \\begin{pmatrix}\n",
    "\\vline & & \\vline\\\\ \n",
    "\\mw[1] & \\dots & \\mw[d] \\\\\n",
    "\\vline & & \\vline\\\\ \n",
    "\\end{pmatrix}\n",
    " \\\\\n",
    "\\E{\\vt{w}_i} &= \n",
    "\\begin{pmatrix}\n",
    "{m_w^{(1)}}_i \\\\\n",
    "\\vdots \\\\\n",
    "{m_w^{(d)}}_i\n",
    "\\end{pmatrix} =  \\mwtilde && \\text{(we define } \\mwtilde \\text{ for shorter notation)}\\\\\n",
    "\\E{||\\vt{w}_i||^2} &= \\E{\\vt{w}_i^T\\vt{w}_i} = Tr(Var(\\vt{w}_i)) + \\E{\\vt{w}_i}^T\\E{\\vt{w}_i}\n",
    "&&\\text{(318 cookbook)}\\\\\n",
    "&=  \\sum_{j=1}^d var(w_{ij}) + ||\\mwtilde[i]||^2 \\\\\n",
    "&=  Tr(\\vt{\\Sigma_w}) + ||\\mwtilde[i]||^2 \\\\\n",
    "\\lp \\vt{W}^T\\vt{W} \\rp_{ij}\n",
    "&= \\sum_{k=1}^d \\lp \\vt{W}^T\\rp_{ik} W_{kj}\n",
    "=\\sum_{k=1}^d W_{ki} W_{kj}\n",
    "= \\sum_{k=1}^d (\\vt{\\tilde{w}}_k)_i (\\vt{\\tilde{w}}_k)_j\n",
    "=  \\sum_{k=1}^d \\lp \\vt{\\tilde{w}}_k \\vt{\\tilde{w}}_k^T \\rp_{ij}\n",
    "\\\\\n",
    "\\E{ \\vt{W}^T\\vt{W} } &= \\sum_{k=1}^d \\E{ \\vt{\\tilde{w}}_k \\vt{\\tilde{w}}_k^T  }\n",
    "= \\sum_{k=1}^d \\lp \\vt{\\Sigma_w} + \\mw \\mw^T \\rp\n",
    "&&\\text{(321 cookbook)}\\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylab import has clobbered these variables: ['sum', 'mean', 'cov']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "%pylab inline\n",
    "import sys\n",
    "if sys.version_info < (3,):\n",
    "    from __builtin__ import sum\n",
    "else:\n",
    "    from builtins import sum\n",
    "import scipy.special as sp\n",
    "from numpy import linalg as LA\n",
    "\n",
    "def hinton(matrix, max_weight=None, ax=None):\n",
    "    \"\"\"Draw Hinton diagram for visualizing a weight matrix.\"\"\"\n",
    "    ax = ax if ax is not None else plt.gca()\n",
    "\n",
    "    if not max_weight:\n",
    "        max_weight = 2**np.ceil(np.log(np.abs(matrix).max())/np.log(2))\n",
    "\n",
    "    ax.patch.set_facecolor('gray')\n",
    "    ax.set_aspect('equal', 'box')\n",
    "    ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "    ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "    for (x, y), w in np.ndenumerate(matrix):\n",
    "        color = 'white' if w > 0 else 'black'\n",
    "        size = np.sqrt(np.abs(w))\n",
    "        rect = plt.Rectangle([x - size / 2, y - size / 2], size, size,\n",
    "                             facecolor=color, edgecolor=color)\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    ax.autoscale_view()\n",
    "    ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def p_norm(X, p=2, axis=0):\n",
    "    \"\"\" Returns the p-norm witouth normalisation ||x_n||^p where n is taken over the specified axis \"\"\"\n",
    "    return np.power(LA.norm(X, ord=p, axis=axis, keepdims=True), p)\n",
    "\n",
    "# a constant that reoccurs often\n",
    "LN2PI =  np.log(2 * np.pi)\n",
    "\n",
    "class BayesianPCA(object):\n",
    "    \n",
    "    def __init__(self, d, N, a_alpha=10e-3, b_alpha=10e-3, a_tau=10e-3, b_tau=10e-3, beta=10e-3):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        q = d - 2\n",
    "        \n",
    "        self.d = d # number of dimensions\n",
    "        self.q = q # max number of components        \n",
    "        self.N = N # number of data points\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.a_alpha = a_alpha\n",
    "        self.b_alpha = b_alpha\n",
    "        self.a_tau = a_tau\n",
    "        self.b_tau = b_tau\n",
    "        self.beta = beta\n",
    "\n",
    "        # Variational parameters\n",
    "        self.means_z = np.random.randn(q, N) # called x in bishop99\n",
    "        self.sigma_z = np.random.randn(q, q)\n",
    "        self.mean_mu = np.random.randn(d, 1)\n",
    "        self.sigma_mu = np.random.randn(d, d)\n",
    "        self.means_w = np.random.randn(q, d) #the means of rows of W are columns here (each column is m_n)\n",
    "                                             #note, this is the transposed definition of <W>\n",
    "        self.sigma_w = np.random.randn(q, q)\n",
    "        self.a_alpha_tilde = np.abs(np.random.randn(1))\n",
    "        self.bs_alpha_tilde = np.abs(np.random.randn(q, 1))\n",
    "        self.a_tau_tilde = np.abs(np.random.randn(1))\n",
    "        self.b_tau_tilde = np.abs(np.random.randn(1))\n",
    "        \n",
    "        self.__update_E_w()\n",
    "        self.__update_E_zn()\n",
    "        self.__update_E_mu()\n",
    "        self.__update_E_wi()\n",
    "        self.__update_E_znW()\n",
    "        self.__update_E_alpha()\n",
    "        self.__update_E_tau()        \n",
    "        \n",
    "    # expectation updates\n",
    "    def __update_E_w(self): \n",
    "        self.E_W = self.means_w\n",
    "        self.E_W_inner = np.einsum(\"ik,jk->ij\", self.means_w, self.means_w) + self.d * self.sigma_w\n",
    "\n",
    "    def __update_E_zn(self):\n",
    "        self.E_zn = self.means_z\n",
    "        self.E_zn_inner = (self.sigma_z.trace() + p_norm(self.E_zn)).T\n",
    "        self.E_zn_outer = np.array([self.sigma_z + np.dot(self.means_z[:,[n]], self.means_z[:,[n]].T) for n in range(self.N)])\n",
    "\n",
    "    def __update_E_mu(self):\n",
    "        self.E_mu = self.mean_mu\n",
    "        self.E_mu_inner = (self.sigma_mu.trace() + p_norm(self.mean_mu))[0]\n",
    "\n",
    "    def __update_E_wi(self):\n",
    "        self.E_wi = self.means_w.T\n",
    "        self.E_wi_inner = (self.sigma_w.trace() + p_norm(self.E_wi)).T\n",
    "\n",
    "    def __update_E_znW(self):\n",
    "        self.E_znW_inner = self.E_W_inner.dot(self.sigma_z).trace() +  np.diag(self.means_z.T.dot(self.E_W_inner).dot(self.means_z)).T\n",
    "\n",
    "    def __update_E_alpha(self):\n",
    "        self.E_alpha = self.a_alpha_tilde / self.bs_alpha_tilde\n",
    "        #self.E_ln_alpha = self.q * (np.log(self.a_alpha) + sp.digamma(self.b_alpha))\n",
    "        self.E_lnp_alpha = self.q * (sp.gammaln(self.a_alpha) - (self.a_alpha - 1)*sp.digamma(self.a_alpha) - np.log(self.b_alpha) + self.a_tau)\n",
    "        self.E_lnQ_alpha = (sp.gammaln(self.a_alpha_tilde) - (self.a_alpha_tilde - 1)*sp.digamma(self.a_alpha_tilde) \\\n",
    "                             - np.log(self.bs_alpha_tilde) + self.a_tau_tilde).sum()\n",
    "        #note: with the current setting of b_alpha, this creates a very high value\n",
    "\n",
    "    def __update_E_tau(self):\n",
    "        self.E_tau = self.a_tau_tilde / self.b_tau_tilde \n",
    "        #self.E_ln_tau = np.log(self.a_tau) + sp.digamma(self.b_tau)\n",
    "        self.E_lnp_tau = sp.gammaln(self.a_tau) - (self.a_tau - 1)*sp.digamma(self.a_tau) - np.log(self.b_tau) + self.a_tau\n",
    "        self.E_lnQ_tau = sp.gammaln(self.a_tau_tilde) - (self.a_tau_tilde - 1)*sp.digamma(self.a_tau_tilde) \\\n",
    "                            - np.log(self.b_tau_tilde) + self.a_tau_tilde\n",
    "    \n",
    "    #parameter updates\n",
    "    def __update_z(self, X):        \n",
    "        # Σ_z = [ I + E(τ) * E(W^T * W) ]^{−1}\n",
    "        self.sigma_z = LA.inv(np.eye( self.q ) + self.E_tau * self.E_W_inner)\n",
    "        \n",
    "        # m^{(n)}_z = E(τ) * Σ_z * E(W^T) * (x_n − E(µ) )        \n",
    "        self.means_z = self.E_tau * self.sigma_z.dot(self.E_W).dot(X - self.E_mu)\n",
    "                \n",
    "        # update expectations\n",
    "        self.__update_E_zn()\n",
    "        self.__update_E_znW()\n",
    "    \n",
    "    def __update_mu(self,X):\n",
    "        # Σµ = (β + N * E(τ) )^{-1} * I\n",
    "        self.sigma_mu = 1.0 / (self.beta + self.N * self.E_tau) * np.eye(self.d)\n",
    "        \n",
    "        # m_µ = E(τ) * Σ_µ * \\sum^N_{n=1} ( (x_n − E(W) * E(z_n))\n",
    "        self.mean_mu = self.E_tau * self.sigma_mu.dot((X - self.E_W.T.dot(self.E_zn)).sum(1, keepdims=True))\n",
    "        \n",
    "        # update expectations\n",
    "        self.__update_E_mu()\n",
    "        \n",
    "    def __update_w(self, X):\n",
    "        # Σ_w = [ diag(E(α)) + E(τ) * \\sum^N_{n=1} E(z_n * z_n^T) ]^{−1}\n",
    "        self.sigma_w = LA.inv( np.diag(self.E_alpha[:,0]) + self.E_tau * self.E_zn_outer.sum() )\n",
    "        \n",
    "        # m^{(k)}_w = E(τ) * Σ_w * \\sum^N_{n=1} E(z_n) * (x_{nk} − E(µ_k) )\n",
    "        self.means_w = self.E_tau * self.sigma_w.dot(self.E_zn.dot(X.T) - N * self.E_mu.T)\n",
    "        \n",
    "        # update expectations\n",
    "        self.__update_E_w()\n",
    "        self.__update_E_wi()\n",
    "        self.__update_E_znW()\n",
    "    \n",
    "    def __update_alpha(self):\n",
    "        # ã_α = a_α + d/2\n",
    "        self.a_alpha_tilde = self.a_alpha + (0.5 * self.d)\n",
    "        \n",
    "        # ~b_{αi} = b_α + E( ||w_i||^2 ) / 2\n",
    "        self.bs_alpha_tilde = (self.b_alpha + (0.5 * self.E_wi_inner) )\n",
    "\n",
    "        # update expectations\n",
    "        self.__update_E_alpha()\n",
    "\n",
    "    def __update_tau(self, X):\n",
    "        # ã_τ = a_τ + N*d/2\n",
    "        self.a_tau_tilde = self.a_tau + (0.5 * self.N * self.d)\n",
    "        \n",
    "        # ~b_τ = b_τ + 1/2 \\sum^N_{n=1} [ ||x_n||^2 + E(||µ||^2) + Tr( E(W^T * W) * E(z_n * z_n^T) ) \n",
    "        #          + 2*E(µ^T)*E(W)*E(z_n) −2*x_n^T*E(W)*E(z_n) − 2x_n^T E(µ) ]\n",
    "        self.b_tau_tilde = self.b_tau \\\n",
    "                            + 0.5 * (p_norm(X).sum()\n",
    "                                         + self.N*self.E_mu_inner \\\n",
    "                                         + self.E_W_inner.dot(self.E_zn_outer).trace(axis2=2).sum() \\\n",
    "                                         + 2*self.E_mu.T.dot(self.E_W.T).dot(self.E_zn).sum() \\\n",
    "                                         - 2*X.T.dot(self.E_W.T).dot(self.E_zn).trace() \\\n",
    "                                         - 2*X.T.dot(self.E_mu).sum()\n",
    "                                     )\n",
    "        \n",
    "        # update expectations\n",
    "        self.__update_E_tau()\n",
    "\n",
    "    def L(self, X):\n",
    "        L = 0.0\n",
    "        \n",
    "        #+ ⟨logp(X|Z,W,μ)⟩  \n",
    "        L += (-0.5) * self.E_tau * \\\n",
    "             ( p_norm(X) - 2*np.diag(X.T.dot(self.E_W.T.dot(self.E_zn)+self.E_mu)).T \\\n",
    "               + 2*self.E_zn.T.dot(self.E_W).dot(self.E_mu) \\\n",
    "               + self.E_znW_inner+self.E_mu_inner ).sum()\n",
    "\n",
    "        #+⟨logp(Z)⟩\n",
    "        L += -0.5 * (self.N*LN2PI + self.E_zn_inner.sum())\n",
    "        \n",
    "        #+⟨logp(W|α)⟩\n",
    "        L += 0.5*( self.d*self.E_lnp_alpha \\\n",
    "                    - self.d*self.q*LN2PI \\\n",
    "                    - (self.E_alpha*self.E_wi_inner).sum() )\n",
    "        \n",
    "        #+⟨logp(α)⟩\n",
    "        L += self.E_lnp_alpha\n",
    "        \n",
    "        #+⟨logp(τ)⟩\n",
    "        L += self.E_lnp_tau\n",
    "        \n",
    "        #+⟨logp(μ)⟩\n",
    "        L += 0.5*( np.power(self.d,2)*np.log(self.beta) \\\n",
    "                    - LN2PI \n",
    "                    - self.beta*self.E_mu_inner )\n",
    "        \n",
    "        #−⟨logQ(Z)⟩\n",
    "        L -= 0.5*self.N * (LA.slogdet(self.sigma_z)[1]\n",
    "                           + self.q*(1+LN2PI))\n",
    "        \n",
    "        #−⟨logQ(W)⟩\n",
    "        L -= 0.5*self.d * (LA.slogdet(self.sigma_w)[1]\n",
    "                           + self.q*(1+LN2PI))\n",
    "    \n",
    "        #−⟨logQ(α)⟩\n",
    "        L-= self.E_lnQ_alpha\n",
    "        \n",
    "        #−⟨logQ(τ)⟩\n",
    "        L-= self.E_lnQ_tau\n",
    "        \n",
    "        #−⟨logQ(μ)⟩\n",
    "        L-=  0.5 * (LA.slogdet(self.sigma_mu)[1]\n",
    "                     + self.d*(1+LN2PI))        \n",
    "        return L\n",
    "    \n",
    "    def fit(self, X, t=1e-4):\n",
    "        p1,p2 = -np.inf, self.L(X)\n",
    "        u = [p1,p2]\n",
    "        du = [p1-p2]\n",
    "        ddu = []\n",
    "        while abs(du[-1]) > t:\n",
    "            self.__update_mu(X)\n",
    "            self.__update_w(X)\n",
    "            self.__update_z(X)\n",
    "            self.__update_alpha()\n",
    "            self.__update_tau(X)\n",
    "            n = self.L(X)\n",
    "            u.append(n)\n",
    "            du.append(p2-n)\n",
    "            ddu.append(p1-2*p2+n)\n",
    "            p1 = p2\n",
    "            p2 = n\n",
    "        print( len(u) )\n",
    "        return np.array(u),np.array(du),np.array(ddu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Learning algorithm (10 points)\n",
    "Implement the learning algorithm described in [Bishop99], i.e. iteratively optimize each of the Q-distributions holding the others fixed.\n",
    "\n",
    "What would be a good way to track convergence of the algorithm? Implement your suggestion.\n",
    "\n",
    "Test the algorithm on some test data drawn from a Gaussian with different variances in orthogonal directions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (10, 100)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAScAAAEBCAYAAAA6r2NqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cc3ae089e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-3780aea916d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mloss_log\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mddu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-57-d02b28a7ba92>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, t)\u001b[0m\n\u001b[0;32m    188\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdu\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__update_mu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__update_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__update_z\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__update_alpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-57-d02b28a7ba92>\u001b[0m in \u001b[0;36m__update_w\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__update_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[1;31m# Σ_w = [ diag(E(α)) + E(τ) * \\sum^N_{n=1} E(z_n * z_n^T) ]^{−1}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigma_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE_alpha\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE_tau\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE_zn_outer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[1;31m# m^{(k)}_w = E(τ) * Σ_w * \\sum^N_{n=1} E(z_n) * (x_{nk} − E(µ_k) )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36minv\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m    524\u001b[0m     \u001b[0msignature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'D->D'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'd->d'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m     \u001b[0mainv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_umath_linalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mainv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36m_raise_linalgerror_singular\u001b[1;34m(err, flag)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Singular matrix\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_raise_linalgerror_nonposdef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "#generate tha bishop data\n",
    "cov = np.tile([5,4,3,2]+[1]*5,(9,1))\n",
    "mean = [0]*9\n",
    "\n",
    "N = 100\n",
    "X = np.random.multivariate_normal(mean, cov, N).T\n",
    "mu = 0\n",
    "sigmas = [5,4,3,2,1,1,1,1,1,1]\n",
    "d=10\n",
    "X = np.zeros((d, N))\n",
    "for i in range(len(sigmas)):\n",
    "   X[i,:] = np.random.normal(mu, sigmas[i], N)\n",
    "\n",
    "PCA = BayesianPCA(*X.shape)\n",
    "print( 'X', X.shape)\n",
    "\n",
    "hinton(PCA.means_w.T)\n",
    "plt.title(\"Hinton plot of $\\langle \\mathbf{W} \\langle $\")\n",
    "plt.show()\n",
    "\n",
    "loss_log,du,ddu = PCA.fit(X)\n",
    "\n",
    "\n",
    "plt.plot(-loss_log)\n",
    "#plt.yscale('log')\n",
    "plt.title('convergence')\n",
    "plt.ylabel('-$\\mathcal{L}$')\n",
    "plt.xlabel('Iterations')\n",
    "plt.show()\n",
    "\n",
    "hinton(PCA.means_w.T)\n",
    "plt.title(\"Hinton plot of $\\langle \\mathbf{W} \\langle $\")\n",
    "#print( PCA.means_w)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. PCA Representation of MNIST (10 points)\n",
    "\n",
    "Download the MNIST dataset from here http://deeplearning.net/tutorial/gettingstarted.html (the page contains python code for loading the data). Run your algorithm on (part of) this dataset, and visualize the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
